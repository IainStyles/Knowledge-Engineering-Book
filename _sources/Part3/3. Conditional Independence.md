# Conditional Independence

In our review of the fundamentals of probability, we reminded ourselves of the notion of *independence*. If a random variable is independent, then the univariate distribution can be factored out of the joint distribution. For examples, given $P(X_1, X_2, X_3)$, then if $X_2$ is independent of $X_1$ and $X_3$, the joint probability distribution can be written as

$$
P(X_1,X_2,X_3) = P(X_2)P(X_2,X_3)
$$

Independence here is *unconditional*: the distribution of $X_2$ is always the same and does not depend on the values of $X_1$ and $X_3$. Examples of independence might include:

* Being good at mathematics is independent of your eye colour.
* Being good at mathematics is not independent of being good at programming, but eye colour is independent of both.
* Height and weight are not independent, but hair colour is indepedent of both

In each of these examples, the distribution of the independent variable does not depend on the values of the other variable(s). This is an extremely limiting condition that will often not be satisified in a real-world problem (although we may choose to assume independence, as we do in naive Bayes for example).

What if two variables are not independent, except under certain conditions? Conditional independence is when variables become independent when conditioned on a differet set of variables. We can express this (in the simple three-variable case) as

$$
P(X_1,X_2\vert X_3) = P(X_1\vert X_3)P(X_2\vert X_3)
$$

Alternatively we can express this as

$$P(X_1\vert X_2,X_3)=P(X_1\vert X_3)$$

**Proof:** 

Given $P(X_1\vert X_2,X_3)=P(X_1\vert X_3)$, use product rule $P(X_1,X_2)=P(X_1\vert X_2)P(X_2)$ to obtain

$$
\frac{P(X_1,X_2,X_3)}{P(X_3)} = \frac{P(X_1,X_3)}{P(X_3)} \frac{P(X_2,X_3)}{P(X_3)}
$$

Multiplying through by $P(X_3)$ and then dividing by $P(X_2,X_3)$ we obtain

$$
\frac{P(X_1,X_2,X_3)}{P(X_2,X_3)} = \frac{P(X_1,X_3)}{P(X_3)}
$$

Finally, by the product rule again, $P(X_1,X_2,X_3)=P(X_1\vert X_2,X_3)P(X_2,X_3)$, we obtain

$$
P(X_1\vert X_2,X_3) = P(X_1\vert X_3)
$$
 (we can use a similar argument to show that $P(X_2\vert X_1,X_3) = P(X_2\vert X_3)$ is also implied).

This permits a slightly different interpretation of conditional independence: $X_2$ does not affect the distribution of $X_1$ when conditioned on $X_3$. Notice that the expression on the right hand side does not involve $X_2$ at all: it is not that $X_2$ is independent, it is that it does not affect the distribution of $X_1$ when conditioning on $X_3$.

Consider the following observation: Taller children are better at reading. This correlation implies that reading ability and height are not independent of each other. Can you think of a reason why height and reading ability might not be independent? There is not obvious biological link between them. However, both height and reading ability are correlated with something else: *age*. Taller children are better at reading because taller children are older, and older children are better at reading, both of which have sound biological causes.

If we condition the joint distribution of height and reading ability on age, then the two distributions will be independent of each other: for 8 year old children, there is no correlation between height and reading ability.

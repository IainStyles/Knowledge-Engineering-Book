{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact Inference in Bayesian Networks\n",
    "\n",
    "Now that we have a Bayesian network, what can we do with it? Well, since the Bayesian network is simply a representation of the full joint probability distribution we can do anything we would have done with the joint distribution. Any quantity that could be calculated from the joint distribution can be computed from the Bayesian network.\n",
    "\n",
    "The straightforward way to do this would simply be to construct the full joint distribution directly, but that would be to miss the point. One of the reasons we use Bayesian networks is to reduce the number of parameters needed to represent the joint distribution and so to construct the full joint distribution from the network would be both wasteful and would also render the efficient and meaning representation provided by the network redundant.\n",
    "\n",
    "However, we don't need to do that, because we can compute anything we can compute from the joint distribution directly from the Bayesian network factorisation. Let's consider again the employment prospects of our school-leavers for which we had that the joint distribution is $P(G,M,F,P)$ which we write as `JointPDF`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "JointPDF = np.zeros([2,2,2,2])\n",
    "JointPDF[0][0][:][:] = np.array([[.066,0.054],[0.0165,0.0135]])\n",
    "JointPDF[0][1][:][:] = np.array([[.081,.099],[.1215,.1485]])\n",
    "JointPDF[1][0][:][:] = np.array([[.044,.036],[.011,.009]])\n",
    "JointPDF[1][1][:][:] = np.array([[.054,.066],[.081,.099]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that this can be factorised as $P(G,M,F,P) = P(P\\vert M)P(F\\vert M)P(M)P(G)$. We'll compute the four terms directly from the `JointPDF` although in practice one would know these independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6 0.4]\n",
      "[0.25 0.75]\n",
      "[[0.8 0.2]\n",
      " [0.4 0.6]]\n",
      "[[0.55 0.45]\n",
      " [0.45 0.55]]\n"
     ]
    }
   ],
   "source": [
    "P_G = JointPDF.sum(axis=(1,2,3))\n",
    "P_M = JointPDF.sum(axis=(0,2,3))\n",
    "P_F_M = np.array([JointPDF.sum(axis=(0,3))[0]/P_M[0],JointPDF.sum(axis=(0,3))[1]/P_M[1]])\n",
    "P_P_M = np.array([JointPDF.sum(axis=(0,2))[0]/P_M[0],JointPDF.sum(axis=(0,2))[1]/P_M[1]])\n",
    "\n",
    "print(P_G)\n",
    "print(P_M)\n",
    "print(P_F_M)\n",
    "print(P_P_M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the compactness of this representation: There are just six independent numbers here.\n",
    "\n",
    "So how do we now derive other quantities from this? We sum up various terms just as we would with the joint distribution. Let consider an example. How would we calculate the distribution $P(F,P)$ from this. From the joint distribution this would be easy, we would do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.245 0.255]\n",
      " [0.23  0.27 ]]\n"
     ]
    }
   ],
   "source": [
    "P_FP = JointPDF.sum(axis=(0,1))\n",
    "print(P_FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is somewhat more complex to do this from the factorised distribution, but only in practical terms; conceptually it is identical:\n",
    "\n",
    "$P(F,P) = \\sum_{M,G}P(G,M,F,P) = \\sum_{M,G}P(P\\vert M)P(F\\vert M)P(M)P(G)$\n",
    "\n",
    "We can simplify this calculation a bit by noting that $G$ is only in one term in the summand and so we can write\n",
    "\n",
    "$P(F,P) = \\sum_{M}P(P\\vert M)P(F\\vert M)P(M)\\cdot\\sum_G P(G)$\n",
    "\n",
    "Noting further that $\\sum_G P(G) = 1$ we have\n",
    "\n",
    "$P(F,P) = \\sum_{M}P(P\\vert M)P(F\\vert M)P(M)$.\n",
    "\n",
    "Taking one term $P_(F=\\mathrm{True},P=\\mathrm{True}) = P(f,p)$ from the joint distribution as an example, we evaluate this as:\n",
    "$P(f,p) = \\sum_M P(p\\vert M)P(f\\vert M)P(M) = P(p\\vert m)P(f\\vert m)P(m) + P(p\\vert \\lnot m)P(f\\vert \\lnot m)P(\\lnot m)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_fm = P_F_M[0,0] * P_P_M[0,0] * P_M[0] + P_F_M[1,0] * P_P_M[1,0] * P_M[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this does indeed correspond to the same quantity calculated directly from the joint distribution. Let's do another example. Can we calculate $P(M\\vert P)$? Yes, sure.\n",
    "\n",
    "$P(M\\vert P) = \\frac{P(M,P)}{P(P)} = \\frac{\\sum_{F,G}P(P\\vert M)P(F\\vert M)P(M)P(G)}{\\sum_{F,M,G}P(P\\vert M)P(F\\vert M)P(M)P(G)}$.\n",
    "We do not need to take this any further to realise that this will take an awful lot of work to get a result. Even the simple joint distribution we computed earlier was hard going: some algebra, plus four multiplications and an addition for one term of the joint distribution. We note that calculating directly from the joint distribution itself requires only a few addition although we would, of course first have to form the joint distribution from the factors which will require a significant number of multiplications. It is possible to generalise the algebraic aspects of this but the arithmetic itself remains.\n",
    "\n",
    "For large problems with many variables this approach is unlikely to be tractable. It can be shown to scale as $O(N2^N)$ which is about as bad as it gets. There are approaches that can accelerate this by removing duplicate calculation but that will only get us to $O(2^N)$ which is still pretty bad. This is not approach we will use in practice for any but the simplest problems.\n",
    "\n",
    "What is the alternative? It is to solve inference problems by *approximate sampling*. This will be our next topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# Introduction to Probabilistic Reasoning

Agents need to be able to handle uncertainty. Their knowledge of the world may be incomplete, or it may be imprecise. The logic-based approaches we have seen in this modules can handle incomplete knowledge by returning multiple possible solutions to a problem (no matter how implausible they may be), but they cannot handle imprecise knowledge.

In the two previous modules you have studied in this course you have seen some simple examples of probabilistic reasoning. These problems have typically involved one or two random variables and you have been able to calculate a range of quantities related to the problem using relatively simple calculations. One of the main challenges with probabilistic reasoning is that as the number of random variables increases, the number of parameters increases much faster that the number of variables. Historically this made large probabilistic reasoning problems computationally intractable. Bayesian networks provide us with a way of dealing with this by allowing us to *factorise* a multivariate joint distribution and thus reduce the number of parameters.

## Recap of definitions from Probability Theory

Let us briefly recall some key ideas and definitions which we will use in this section. These definitions are for discrete probability distributions; similar definitions exist for continuous distributions.

### The Joint Distribution

The joint distribution of random variables $X_1, X_2,\dots, X_N$ is written as 

$P(\mathbf{X}) = P(X_1, X_2,\dots, X_N)$

With this notation, the probability that $X_1=x_1 \land X_2=y_2 \land \dots\land X_N=x_N$ is

$P(\mathbf{X=\mathbf{x}}) = P(X_1=x_1, X_2=x_2,\dots, X_N=x_n)$.

### Marginalisation and the Sum Rule

If we wish to know the overall distribution of a subset of the random variables, we *marginalise* over the other variables. For example:

$P(X_j, X_k) = \sum_{i\neq j,k} P(X_1, X_2,\dots,X_N)$

### Conditional Probability

If we wish to know the distribution of a subset of the random variables at a *fixed value* of the other random variables, we compute the *conditional* probability as (for a two-variable example)

$P(X_i\vert X_j) = \frac{P(X_i, X_j)}{P(X_j)}$

### The Product Rule
The product rule for two variable is written as:

$P(X_i,X_j) = P(X_i\vert X_j)P(X_j)$

This is a *factorisation* of the joint distribution.

### Independence
If our random variables are all *independent* of each other (their distributions do not depend on the values of the other variables, then the joint distribution can be written as a product of univariate distributions):

$P(\mathbf{X}) = \prod_{i=1}^N P(X_i)$


### Bayes Rule

The definition of condition probability $P(X_i\vert X_j) = \frac{P(X_i, X_j)}{P(X_j)}$, combined with the symmetry of the joint distribution $P(X_i,X_j) = P(X_k,X_i)$ leads directly to Bayes Rule:

$P(X_i\vert X_j) = \frac{P(X_j\vert X_i)P(X_i)}{P(X_j)}$.


## Parametric explosion

A probabilistic model of tossing a coin requires just one parameters - $P(H)$. $P(T)$ would do just as well because $P(H)+P(T)=1$. There are only two outcomes and one of them must happen. If you toss two coins, you will need at most two parameters to describe their joint distribution: $P(\mathrm{coin1}=\mathrm{heads})$ and $P(\mathrm{coin2}=\mathrm{heads})$ (if you are tossing the same coin twice these two parameters will be the same.) This is because coin tosses are **independent** events and so the joint distribution can be factorised as

$$
P(X,Y) = P(X)P(Y)
$$

Rolling dice can be treated in the same way, except each die require five parameters to specify its distribution. Even if each die has a different bias, adding another die only adds another five parameters and the number of parameters scales linearly with the number of die.

If the events are not independent then the problem becomes very much more complex. The number of parameters in the joint distribution of a discrete variable problem is the number of entries in its joint distribution, minus one because the final entry can be derived from the others plus the normalisation constraint. For a problem with $N$ non-independent random variables, there are therefore up to $N(N-1)/2 - 1$ parameters in the joint distribution (the joint distribution is symmetric). This is quadratic in the number of variables. The same is true for continuous random variable where the size of the covariance matrix scales similarly. In any case, it is very rare that in practice we will have access to the full joint distribution: what we observe in the world is almost exclusively samples from a range of conditional distributions. If we knew all the conditional distributions then we could use the product rule to construct the joint distribution, but the full set of conditional distributions needed to do this are almost never directly accessible to us.

In real-world problems, the variables are rarely truly independent. One expects, for example, the financial performance of a retail company to be affected by a whole range of factors, perhaps including:
- Interest rates - affect how much money people need to pay for their mortgage and so reduces the amount available to spend in shops.
- Inflation - affects the prices in the shops.
- Weather - affects the number of shoppers who go to stores.

These are clearly not all independent - inflation and interest rates are certainly related to each other (because central banks manipulate interest rates to control inflation), but perhaps weather *is* independent of the other two factors? And perhaps under some circumstances (for example, online retail), weather does not affect financial performance. This may lead us to suspect that perhaps a partial factorisation of the joint distribution of these variable might be possible - and it often is. The key concept that will enable this is the notion of *conditional independence*.


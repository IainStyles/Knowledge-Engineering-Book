# Introduction to Logic in Artificial Intelligence

Logic is central to computer science. Many of you will be familiar with its role in the design and construction of computers. Logic gates and the principles of digital logic are the building blocks of modern computer technology. The principles of logic though, have much deeper roots in computer science. At a very fundamental level, the [Curry-Howard Correspondence](https://en.wikipedia.org/wiki/Curry–Howard_correspondence) establishes that there is a deep and poorly understood link between logic and computation, and more specifically that computation is *equivalent* to logic, with logical propositions corresponding to *types*, and logical proofs to computer programs. Logic is also fundamental to the [Formal Methods](https://en.wikipedia.org/wiki/Formal_methods) that a rewidely used to specify and verify computer programmes. It has also had, and will continue to hold, a central role in the development of **Artificial Intelligence**. This will be the subject of this part of the course.

In the current era in which pure data-driven approaches using machine learning and deep learning dominate the state of the art in artificial intelligence, it may seem arcane and pointless to study logic. This point of view would be mistaken. Perhaps the fundamental tenet of AI is the notion that can construct an inanimate physical device that can execute a set of operations that can mimic human thought. Consider the following two statements:

> - Aristotle is a man
> - All men are mortal

Most humans are easily able to infer that if these two statements are true, then Aristotle must be mortal. This type of thinking - *reasoning* - is widely believed to be a central component of any intelligent agent. Such reasoning abilities are demonstrated in a wide range of intelligent biological agents. For example, see [this video](https://www.youtube.com/watch?v=s2IBayVsbz8) of crows using stones to raise the level of water in a jar so they can get to some food that is floating in the water. Reasoning abilities that allow rigourous arguments to be constructed that enable decisions to be made and actions to be taken to acheive  goals are generally regarded as an essential component of any intelligent system, though there are many disagreements about how to endow an artificial intelligent system with such abilities.

If we accept that reasoning is an essential part of intelligence, then an artificial intelligence requires that the processes of reasoning that emerge naturally in biological systems need to be *mechanised* for and engineered implementation in a different medium. The mechanisation, or *automation* of reasoning has been studied for millenia, with philophers from al of the ancient cultures developing formal approaches to constructing arguments. Perhaps most famous in Western cultures is the work done by the Greek mathematician Euclid in his famous work *Elements*, but equally important work was being done in other cultures, for example, by al-Khwārizmī, from the ancient region of Khwarazm in what was once part of Iran and is part of Uzbekistan/Turkmenistan region, whose work included the development of modern algebra and the concept of the *algorithm*, a notion named after him. Their work is a direct precursor of first the work of Leibniz, Hobbes, and Descartes in the 17th Century, who investigated, from different perspectives, the idea that rational thought could be reduced to calculation.

The study and automation of correct reasoning was also being extensively studies in philosophy, with formal logic crossing the boundary to mathematics where Alfred North Whitehead and Bertrand Russell successfully laid logical foundations to the whole of mathematics in their seminal work *Principia Mathematica*. A very accessible and informative introduction to Russell's work can be hear on BBC Radio 4's [In Our Time](https://www.bbc.co.uk/sounds/play/b01p8fsr). Formal logic would later play a critical role in the development of the first computer-based intelligent systems.

The notion that computers could be intelligent was really catalysed by the man who is widely regarded as the Godfather of AI, Alan Turing. Turing first put forward the notion that in additional to having reasoning abilities, humans also used available information - knowledge - to solve problems {cite:p}`turing1950computing`. This idea heavily influenced other early pioneers of AI such as John McCarthy, Marvin Minsky, Herb Simon, and others who realised that reasoning could be performed by *symbol manipulation*, where the symbols represented pieces of knowledge and the manipulations represented the steps in a reasoned argument. This focus on *symbolic AI* lasted several decades and resulted in systems such as Newell, Shaw, and Simon's *Logic Theorist* {cite:p}`newell1956logic`which was able to prove thirty-eight of the first fifty-two theorems in Whitehead and Russell's *Principia Mathematica*.

These early successes were compelling and drove widespread optimism about AI-by-symbol-manipulation, but it was not too long before problems became evident. Foremost among these was the practical difficulty of maintaining a large and rapidly growing *knowledge base* in which known facts are represented in a *structured way*. This has led, in recent years, to a refocussing ofm AI research onto machine learning methods, and most recently deep neural networks. These methods have been successful across a wide range of problems and this includes demonstration of some *limited* reasoning capabilities. In particular, large language models (LLM), which learn from enormous *unstructured* knowledge bases, have been shown to be able to perform some basic reasoning. However, even cursory evaluations of the output of the most sophisticated LLM reveal that they often make basic errors in reasoning. This is because they have not been endowed, by design, with a mechanism for performing such reasoning *rigourously* and instead rely on correct reasoning emerging from statistical correlations in their dataset. Importantly, they do not have a robust internal representation of known facts about the world, although it is highly likely that they are likely to have developed some kind of representation.

In this part of the course we will focus on developing methods for *formally* representing knowledge and reasoning (making decisions) *rigourously* over that knowledge. Our main approach will be to use the principles of formal *logic*, which we will find to be a very powerful tool for both manual and automated reasoning. The methods we will use were the mainstay of symbolic AI, and it is widely believed that the current challenges in getting LLMs to reason rigourously could be addressed by embedding logical capabilities into the LLM model.

Let's begin with an exercise - how good is *your* logical reasoning?

<!--## References-->
```{bibliography}
```

<!--
* The moves that chess pieces can make
* The mapping from DNA sequences to proteins
* 

We will be interested in
i. Representing a knowledge base (set of facts) 
ii. Reasoning about those facts (inference).

As a trivial, *informal* example of what we are trying to do, consider the following two statements:

* The sun only shines during the day
* The sun is shining

What third fact can you infer from these statements? We can all do this without really thinking about is. If we had more facts, or more complex facts, it is far more challenging. Some of you may be familiar with the well-known "logic puzzles". Here is a simple example.

Ahmed, Niamh, and Fei are three friends who have gone for dinner together. Can you match each of then with what they ordered?

* Ahmed ordered a main course of Falafel but did not order Cheesecake for dessert.
* The person who ordered Ice Cream for dessert did not order Irish Stew for their main course.

We can solve this problem (try it!) but we have to work quite hard and it is easy to see how we might make a mistake, but this is still a rather simple problem. Imagine now that we have tens, hundreds, thousands or more facts. We would not be able to make inferences over such a knowledge base manually, but we can automate it. In this part of the module, we will learn how to represent and reason about a knowledge base using logic.

** REPRODUCE WUMPUS FOR THIS EXAMPLE, ILLUSTRATE THE KEY CONCEPTS OF
* SENTENCE
* KNOWLEDGE BASE 
* ENTAILMENT
* MODEL/WORLD
* KNOWLEDGE BASE TRUTH/FALSITY
* LOGICAL INFERENCE - START WITH MODEL CHECKING
* SOUNDNESS
* COMPLETENESS
* GROUNDING/LEARNING

Introduce the concepts and the problem in the recording

Use a simple 3-person dinner-dessert example
First solve it informally (do this interactively)
Then solve it by enumerating the worlds and applying model checking (get them to do this)

Ahmed Niamh Chen
Daal Falafel Stew
IceCream Cheesecake ApplePie

27 possible world models
1. Ahmed Daal IceCream
2. Ahmed Daal Cheesecake
3. Ahmed Daal ApplePie
4. Ahmed Falafel IceCream
5. Ahmed Falafel Cheesecake
6. Ahmed Falafel ApplePie
7. Ahmed Stew IceCream
8. Ahmed Stew Cheesecake
9. Ahmed Stew ApplePie
10. Niamh Daal IceCream
11. Niamh Daal Cheesecake
12. Niamh Daal ApplePie
13. Niamh Falafel IceCream
14. Niamh Falafel Cheesecake
15. Niamh Falafel ApplePie
16. Niamh Stew IceCream
17. Niamh Stew Cheesecake
18. Niamh Stew ApplePie
19. Chen Daal IceCream
20. Chen Daal Cheesecake
21. Chen Daal ApplePie
22. Chen Falafel IceCream
23. Chen Falafel Cheesecake
24. Chen Falafel ApplePie
25. Chen Stew IceCream
26. Chen Stew Cheesecake
27. Chen Stew ApplePie

The true model:
7. Ahmed Stew IceCream
11. Niamh Daal Cheesecake
24. Chen Falafel ApplePie

The clues
1. The person who had the Stew did not have the ApplePie (eliminates worlds 9, 18,27)
2. Niamh had the Daal (eliminates 1,2,3,13--21 and must include one of 10, 11, 12)
3. Ahmed did not have Cheesecake (eliminates 5, 8)
4. Chen had the ApplePie (eliminates 6,12,19,20,22,23,25,26 as it means she did not have Stew)
5. The person who had the Ice Cream did not have the Falafel (eliminates 4 and then 10)

Let's work through the clues:

Clue 1 immediately eliminates worlds 9, 18, and 27.
Remaining models: 1--8, 10-17, 19--26.

Clue 2 immediately eliminates worlds 1--3, 13--21, and tells us that one of 10, 11, and 12 must be true.
Remaining models: 4--8, 10--12, 22-26.

Clue 3 eliminates worlds 5 and 8 (and also 2 which had already been eliminated)
Remaining models: 4, 6, 7, 10--12, 22-26

Clue 4 eliminates worlds 6, 12, 22, 23, 26, and when combined with clue 1, also eliminates world 25. 
Remaining worlds: 4, 7, 10, 11, 24

Clue 5 eliminates world 4, and then world 10 (because Ahmed must have had the ice cream)
Remaining worlds: 7, 11, 24


(Can you get ChatGPT to solve this? I couldn't)

## Propositional Logic
Perhaps the most familiar and simplest form of logic is propositional logic. 

### Syntax
There are five **logical connectives** commonly used in propositional logic.
* $\lnot$
* $\lor$
* $\land$
* $\implies$
* $\iff$


### Semantics
### Entailment

## Propositional Theorem Proving
-->

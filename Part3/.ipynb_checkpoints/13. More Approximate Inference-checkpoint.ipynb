{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Approximate Inference\n",
    "\n",
    "We have previously introduce the *discrete sampling* method and shown how it can be adapted for conditional distributions using *rejection sampling*, in which we sample from across the parameters space and then only retain those samples which have the correct values of the *evidence variables*: variables that are considered to be fixed by our accumulated knowledge. For example, if we wished to evaluate the distribution $P(M\\vert P=True, G=False)$ we would allow the sampler to generate events with $P$ both True and False, and then reject all those samples where $P$ is False.\n",
    "\n",
    "## Likelihood weighting\n",
    "\n",
    "An alternative approach is to only generate samples that are consistent with the evidence, that is, that have the same value as the evidence variables. This means that no events are wasted which is good. However, events should not necessarily be treated as \n",
    "\n",
    "* Fix all the evidence variables to their observed values\n",
    "* Sample non-evidence variables conditioned on the fixed evidence variables\n",
    "* Compute the probability that the sample you have just drawn would give rise to the evidence that you have fixed.\n",
    "* Include your sample in the trace, weighted by this probability.\n",
    "\n",
    "Considering our example:\n",
    "* Fix $P=True$.\n",
    "* Set weight $w=1.0$.\n",
    "* $G$ is an evidence variable and is fixed at False. Set $w\\leftarrow w\\times P(G=False) = 0.4$.\n",
    "* $M$ is not evidence. Sample from $P(M) = (0.25,0.75)$, suppose this returns True.\n",
    "* Sample from $P(F\\vert M=True) = (0.4,0.6)$, suppose this returns False.\n",
    "* $P$ is an evidence variable and is True. Set $w\\leftarrow w\\times P(P=True\\vert M=True) = 0.4\\times 0.55 = 0.22$.\n",
    "* Record (False, True, False, True) with weight 0.22.\n",
    "\n",
    "Why does this work? Firstly, all of the samples generate have the observed values of the evidence variables and so all of the samples can be used. This give improved accuracy results or faster convergens for a given level of accuracy. Secondly, it can be shown rigourously that weighting of the likelihoods in this way is fully consistent with direct sampling from the desired posterior distribution. A proof can be found in Section 14.5.1 of Russell and Norvig (Fifth Edition).\n",
    "\n",
    "\n",
    "## The Gibbs Sampler\n",
    "\n",
    "Discrete sampling and its variants all generate samples independently. A new sample in the trace has no knowledge of any previous samples that have been generated. A very common alternative treats the sampling as a *Markov Chain* in which the generation of the next sample depends on the current sample, but not on events prior to the current sample. Markov Chain Monte Carlo (MCMC) sampling methods generate each successive sample by randomly modifying the current sample. There are many different MCMC methods but one that is well-suited for Bayesian networks is the **Gibbs Sampler** named after the Statistical Physicist Josiah Willard Gibbs, whose work in statistical thermodynamics has some close parallels with (and significantly predates) this approach to random sampling.\n",
    "\n",
    "Gibbs sampling is conceptually very simple.\n",
    "\n",
    "* Begin by fixing all *evidence variables* to their observed values. These will not be changed through the sampling process.\n",
    "* Initialise all of the non-evidence variables to random values. These initial values must be supported by the distribution attached to the variable but there are no other requirement of this initialisation.\n",
    "* A new state is created by sampling *one* of the non-evidence variables, chosen at random. The sampling of the chosen variable will be from its conditional probability distribution which is implicitly or explicitly conditioned on its *Markov Blanket* which is the set of variables that are either parents or children of the chosen variable, or are co-parents of the variable's children. It can be proven that any variable is conditionally independent of all variables outside of its Markov blanket and so we only need to know the local neighbourhood. For small systems, this concept seems rather unecessary, but for Bayesian networks with large number of variables this notion helps to improve the efficiency of the computation.\n",
    "* The Gibbs sampler determines its outcome by counting how many times it visits each state and normalising. If it visits a state where $M$ is True forty-five times, and $M$ is False fifteen times, the resulting distribution is $P(M\\vert F,P)=(0.25,0.75)$. \n",
    "\n",
    "The Gibbs sampler is a classical Markov Chain and can be analysed in terms of dynamical systems theory. We will not go in to this in detail here, but we will state the main finding of the analysis: the sampling process settles into an equilibrium in which the the long-run fraction of time that the sampler spends in each state is exactly proportional to that state's posterior probability. This results from the transition probabilities between states being defined by the Markov blankets of the non-evidence variables.\n",
    "\n",
    "An important aspect of working with the Gibbs samples is that it needs to settle down to that equilibrium state to yield accurate results. In the early stages of the sampling, the samples are not necessarily drawn in proportion to their posterior probability because the initially random state is not yet fully consistent with the sampling distribution. It is therefore very common for Gibbs samplers to require a *burn-in* period to ensure they have reached equilibrium before we start counting states.\n",
    "\n",
    "We do not typically implement Gibbs samplers from first principles as they are quite tricky to implement and can be sensitive to numerical precision issues. Fortunately the library `pymc3` provides us with a tried-and-trusted implementation that we can just use. Let's build a network in PYMC3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmc\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymc'"
     ]
    }
   ],
   "source": [
    "import pymc3 as mc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mc.Model() as model:\n",
    "    German = mc.Bernoulli('German',0.4)\n",
    "    Math = mc.Bernoulli('Math',0.75)\n",
    "    P_Finance = mc.Deterministic('P_Finance', mc.math.switch(Math,0.6,0.2))\n",
    "    Finance = mc.Bernoulli('Finance',P_Finance)\n",
    "    P_Programming = mc.Deterministic('P_Programming', mc.math.switch(Math,0.55,0.45))\n",
    "    Programming = mc.Bernoulli('Programming',P_Programming)\n",
    "    step = mc.Metropolis()\n",
    "    trace = mc.sample(10000, step=step, tune=5000, random_seed=123, progressbar=True)  # init=start,\n",
    "\n",
    "mc.traceplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in trace:\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dictionary = {\n",
    "    'German': [1 if ii else 0 for ii in trace['German'].tolist()],\n",
    "    'Math': [1 if ii else 0 for ii in trace['Math'].tolist()],\n",
    "    'P_Finance': [ii for ii in trace['P_Finance'].tolist()],\n",
    "    'Finance': [1 if ii else 0 for ii in trace['Finance'].tolist()],\n",
    "    'P_Programming': [ii for ii in trace['P_Programming'].tolist()],\n",
    "    'Programming': [1 if ii else 0 for ii in trace['Programming'].tolist()]\n",
    "}\n",
    "df = pd.DataFrame(dictionary)\n",
    "df.head()\n",
    "\n",
    "P_Math_Finance = float(df[(df['Finance'] == 1) & (df['Math'] ==1)].shape[0]) / df[df['Finance'] == 1].shape[0]\n",
    "print(P_Math_Finance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

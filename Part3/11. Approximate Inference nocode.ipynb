{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference\n",
    "\n",
    "We have recently demonstrated that exact inference is rather non-trivial. It's conceptually straightforward, but practically difficult and very expensive, scaling very badly with the number of variables. It also **fails completely** for continuous variables where the calculations involved are essentially intractable. For these reasons it very rare to use this approach in practice. So what do we do instead? The dominant class of algorithm for inference on Bayesian networks is based on *sampling* and we will now study two approaches to sampling. The first is the *direct sampling* method which is straightforward but inefficient. The second is *Markov Chain sampling* and specifically *Gibbs sampling*. This is the dominant method in the practical application of Bayesian networks.\n",
    "\n",
    "## Direct Sampling\n",
    "\n",
    "Imagine we have a coin and we want to know its distribution of heads vs tails if we toss it. How can we do this? We cannot determine the distribution easily from first principle, although we typically make some assumptions about the symmetry of the coin to conclude that $P(H)=P(T)=0.5$. How could we verify that our assumptions were correct though?\n",
    "\n",
    "There is a very simple answer to this. We *sample* the distribution by tossing the coin a large number of times ($N$) and counting how many times each of the two outcomes occurs ($N_H, N_T$). We then estimate the distribution as $P(H)=N_H/N$ and $P(T)=N_T/N$. If the number of heads is roughly equal to the number of tails then we have approximately verified that that coin is fair and that $P(H)=P(T)$. We can simulate a fair coin using the Binomial Distribution with $p=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling can provide very accurate estimate of a distribution provided sufficiently many samples are taken. Here's another example of a Monte Carlo Pi estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How do we apply this idea to Bayesian networks, where we have a chain of conditional distributions? Let's illustrate via an example.\n",
    "\n",
    "Consider our \n",
    "\n",
    "\n",
    "* Draw one sample from $P(G) = (0.4,0.6). Answer is (say) False.\n",
    "* Draw one sample from $P(M) = (0.75,0.25). Answer is (say) True.\n",
    "* Draw one sample from $P(F\\vert m) = (0.8,0.2)$. Answer is True.\n",
    "* Draw one sample from $P(P\\vert m) = (0.55,0.45)$. Answer is False.\n",
    "\n",
    "We have now drawn one sample from the joint distribution and obtained (False,True,True,False). Given a large number of such samples, we can compute anything we want. Let's try it out. We will code this up very explicitly which will be slightly inefficient but we want to maintain transparency whilst we are learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some basic stats on the Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! Can we now estimate some other quantities? For example, how about estimating $P(M\\vert F)$?\n",
    "\n",
    "First of all, we can do this by hand via Bayes' theorem: $P(M\\vert F) = (F\\vert M)P(F)$. We know all these quantities so we can calculate $P(m\\vert f) = P(f\\vert m)P(m)/p(f) = 0.6\\times 0.75/0.5 = 0.9$.\n",
    "\n",
    "How do we get this from the trace?\n",
    "\n",
    "To find $P(p\\vert f)$ we need to\n",
    "* Find the rows in the Trace where the condition $f$ is True.\n",
    "* Then compute the proportion of those rows for which $p$ is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of **rejection sampling**. We have thrown away all of the rows in the Trace for which $F$ was false. We may consider this to be a waste; however, if we want to ask general question of the distribution, the full trace in which all variables are drawn allows us to do so.\n",
    "\n",
    "Rejection sampling is very general and it can be shown that the standard deviation in the error in each probability is proportional to $1/\\sqrt{N}$. However, if we have specific questions we want to answer and are time or compute-limited, then we may wish for something more efficient. This will be our next topic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
